{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (0.2.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/lib64/python3.6/site-packages (from lime) (0.23.2)\n",
      "Requirement already satisfied: tqdm in /usr/lib/python3.6/site-packages (from lime) (4.45.0)\n",
      "Requirement already satisfied: numpy in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from lime) (1.19.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from lime) (0.17.2)\n",
      "Requirement already satisfied: matplotlib in /usr/lib64/python3.6/site-packages (from lime) (2.2.5)\n",
      "Requirement already satisfied: scipy in /usr/lib64/python3.6/site-packages (from lime) (1.3.3)\n",
      "Requirement already satisfied: networkx>=2.0 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2.5.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/lib64/python3.6/site-packages (from scikit-image>=0.12->lime) (7.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2020.9.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/lib64/python3.6/site-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.6/site-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3.6/site-packages (from matplotlib->lime) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from matplotlib->lime) (2.8.1)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3.6/site-packages (from matplotlib->lime) (2020.5)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3.6/site-packages (from matplotlib->lime) (1.14.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib64/python3.6/site-packages (from matplotlib->lime) (1.2.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
      "Requirement already satisfied: tabulate in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (0.8.9)\n",
      "Requirement already satisfied: pdfkit in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lime --user\n",
    "!pip install tabulate --user\n",
    "!pip install pdfkit --user\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "\n",
    "from lime import lime_tabular\n",
    "\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lime\n",
    "import pickle\n",
    "\n",
    "PICKLES_PATH = \"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/classification/pickles\"\n",
    "SUBSETS_PATH = \"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/subsets_useful\"\n",
    "EXPLANATIONS_PATH = \"/afs/inesc-id.pt/home/aof/public_html\"\n",
    "\n",
    "BOOTSTRAP = '<head><link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\" integrity=\"sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO\" crossorigin=\"anonymous\"></head>'\n",
    "#=================================================================================\n",
    "'''\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(iris.data, iris.target, train_size=0.80)\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train, labels_train)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    exp = explainer.explain_instance(test[i], rf.predict_proba, num_features=3, top_labels=2)\n",
    "    print(vars(exp))\n",
    "    exp.show_in_notebook(show_table=True, show_all=True)   \n",
    "    #break\n",
    "    print(vars(exp.domain_mapper))\n",
    "    break\n",
    "'''\n",
    "\n",
    "EXPLANATIONS = {\n",
    "    'f0': 'The number of open/close cycles of the glottis.',\n",
    "    'hnr': 'Measures the frequency variation between cycles. Affected by lack of control on the vocal cords vibration.',\n",
    "    'jitter': 'Measures the amplitude variation between cycles.',\n",
    "    'shimmer': 'The ratio between periodic (associated with normal speed production) and non-periodic (associated with noise) speech components.',\n",
    "    'mfcc': 'Features that approximate to our perception of the audio quality.',\n",
    "    'plp': 'Features that approximate to our perception of the audio quality.',  \n",
    "}\n",
    "\n",
    "\n",
    "NORMAL_VALUES = {\n",
    "    'f0': '105-160 (male) | 175-245 (Female)',\n",
    "    'hnr': '< 1.04',\n",
    "    'jitter': '< 3.81',\n",
    "    'shimmer': '< 40',\n",
    "    'mfcc': '',\n",
    "    'plp': '',  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_html_to_file(html, path):\n",
    "    f = open(path, \"w\")\n",
    "    f.write(html)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_explanation_text(feature):\n",
    "    return get_explanation_or_values(feature, EXPLANATIONS)\n",
    "\n",
    "def get_normal_values(feature):\n",
    "    return get_explanation_or_values(feature, NORMAL_VALUES)\n",
    "\n",
    "def get_explanation_or_values(feature, obj):\n",
    "    lower = feature.lower()\n",
    "    \n",
    "    for key in obj:\n",
    "        if key in lower:\n",
    "            return obj[key]\n",
    "    \n",
    "    print(\"No explanation or normal value found for feature {}\".format(feature))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Model: MLPClassifier instance\n",
    "    Patient_Lines: lines of CSV corresponding to the patient to be tested\n",
    "    Train: Training fractionf of the CSV\n",
    "    Feature_names: list of features (columns of csv)\n",
    "    Output path: file path to write the output, None for writing on screen\n",
    "'''\n",
    "def explain_patient(experiment, patient, model, patient_lines, train, feature_names, params, output_path=None):\n",
    "    \n",
    "    num_features = 5\n",
    "    \n",
    "    #Build tabular\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=feature_names, class_names=['HC', 'PD'], discretize_continuous=True)\n",
    "\n",
    "    # Pass each line to LimeTabularExplainer\n",
    "    explanations = []\n",
    "    for i, row in patient_lines.iterrows():\n",
    "        #Run only 1 in every 10 time frames (10 samples / second)\n",
    "        if i % 10 == 0:\n",
    "            explanations.append(explainer.explain_instance(row, model.predict_proba, num_features=num_features, top_labels=1))\n",
    "\n",
    "    print(\"Got {} explanations\".format(len(explanations)))\n",
    "    # Sum all local_exp\n",
    "    total_local_exp = {}\n",
    "    for explanation in explanations:\n",
    "        #local_exp = {1: [(12, 0.139704949653412), (44, 0.10278682378094679)]}\n",
    "        local_exp = explanation.local_exp\n",
    "        \n",
    "        # will produce {<feature>: (count, sum_weight), ...\n",
    "        for tup in local_exp[0 if 0 in local_exp else 1]:\n",
    "            #tup = (12, 0.139704949653412)\n",
    "            feature = feature_names[tup[0]]\n",
    "            if feature in total_local_exp:\n",
    "                total_local_exp[feature][0] += 1\n",
    "                total_local_exp[feature][1] += tup[1]\n",
    "            else:\n",
    "                total_local_exp[feature] = [1, tup[1]]\n",
    "    \n",
    "    #Average local_exp\n",
    "    # Key --> feature\n",
    "    # value --> average value\n",
    "    averaged_local_exp = {}\n",
    "\n",
    "    for key in total_local_exp.keys():\n",
    "        tup = total_local_exp[key]\n",
    "        averaged_local_exp[key] = tup[1] / tup[0]\n",
    "\n",
    "    # Normalize predict_proba\n",
    "    average_predict_proba = [sum(items) for items in zip(*[explanation.predict_proba for explanation in explanations ])]\n",
    "    average_predict_proba /= np.float64(sum(average_predict_proba))\n",
    "\n",
    "    result = display_or_save_patient(averaged_local_exp, average_predict_proba, output_path, num_features, patient)\n",
    "    f = open(\"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/done_patients/{}/{}_{}.txt\".format(experiment, generate_suffix(params), patient.split(\"_\")[0]), \"w\")\n",
    "    f.write(\"Done\")\n",
    "    f.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suffix(params):\n",
    "    return \"{}_{}_{}_{}\".format(params['solver'], params['alpha'], params['max_iter'], params['activation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "def display_or_save_patient(averaged_local_exp, average_predict_proba, output_path, num_features, patient):\n",
    "    prediction_proba = \\\n",
    "            '<table class=\"table\" style=\"margin: 0 auto;\"> \\\n",
    "                <thead> \\\n",
    "                    <tr> \\\n",
    "                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">HC</td> \\\n",
    "                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">PD</td> \\\n",
    "                    </tr> \\\n",
    "                </thead> \\\n",
    "                <tr> \\\n",
    "                    <td style=\"margin: 2px; text-align: center;\">{}</td> \\\n",
    "                    <td style=\"margin: 2px; text-align: center;\">{}</td> \\\n",
    "                </tr> \\\n",
    "            </table>' \\\n",
    "            .format(\"{:.2f}\".format(average_predict_proba[0]), \"{:.2f}\".format(average_predict_proba[1]))\n",
    "    \n",
    "    averaged_local_exp = {k: v for k, v in sorted(averaged_local_exp.items(), key=lambda item: -abs(item[1]))}\n",
    "    feature_weights = averaged_local_exp.items()\n",
    "    feature_weight_table = '<table class=\"table\" style=\"margin: 0 auto;\"> \\\n",
    "                                <thead> \\\n",
    "                                    <tr> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Feature</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Relevance weight</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Healthy values</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Description</td> \\\n",
    "                                    </tr> \\\n",
    "                                </thead>'\n",
    "\n",
    "    accounted = 0\n",
    "    for feature_weight in feature_weights:\n",
    "        s = '<tr> \\\n",
    "                <td style=\"margin: 2px; text-align:center; font-weight: bold;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "            </tr>' \\\n",
    "            .format(feature_weight[0], feature_weight[1], get_normal_values(feature_weight[0]), get_explanation_text(feature_weight[0]))\n",
    "        feature_weight_table += s\n",
    "        \n",
    "        accounted += 1\n",
    "        if accounted == num_features:\n",
    "            break\n",
    "\n",
    "    feature_weight_table += '</table>'\n",
    "    \n",
    "    #for feature_index in averaged_local_exp:\n",
    "    #    feature_weight.append([feature_index, averaged_local_exp[feature_index]])\n",
    "    \n",
    "    \n",
    "    table = '<table class=\"table\" style=\"margin: 0 auto;\"> \\\n",
    "                <thead> \\\n",
    "                    <tr> \\\n",
    "                        <td style=\"width: 45%; margin: 2px; text-align:center; font-weight: bold;\">Prediction Probabilities</td> \\\n",
    "                        <td style=\"width: 45%; margin: 2px; text-align:center; font-weight: bold;\">Feature weight</td> \\\n",
    "                    </tr> \\\n",
    "                </thead> \\\n",
    "                <tr> \\\n",
    "                    <td style=\"width: 45%; margin: 2px; text-align:center\">{}</td> \\\n",
    "                    <td style=\"width: 45%; margin: 2px; text-align:center\">{}</td> \\\n",
    "                </tr> \\\n",
    "            </table>' \\\n",
    "        .format(prediction_proba, feature_weight_table)\n",
    "\n",
    "    if output_path == None:\n",
    "        display(HTML(table))\n",
    "    else:\n",
    "        write_html_to_file(BOOTSTRAP + table, output_path)\n",
    "        \n",
    "    dic = {'name': patient.split(\"_\")[0], 'hc': average_predict_proba[0], 'pd': average_predict_proba[1], 'features': [json.dumps(averaged_local_exp)]}\n",
    "    try: \n",
    "        results = pd.DataFrame(dic, index=None)\n",
    "    except: \n",
    "        message = traceback.format_exc()\n",
    "        print(message)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlp_params_list(dry_run=False):\n",
    "    if (dry_run):\n",
    "        return {\n",
    "            \"independent_200\": [\n",
    "                { # Got it\n",
    "                    'dataset': 'gita',\n",
    "                    'solver': 'lbfgs',\n",
    "                    'activation': 'tanh',\n",
    "                    'alpha': 0.0001,\n",
    "                    'max_iter': 2000,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    return {\n",
    "        \"baseline\": [\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "        ],\n",
    "        'baseline_200': [\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "        ],\n",
    "        'semi': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'fralusopark_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita_fralusopark',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita_fralusopark',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita_fralusopark',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "        ],\n",
    "        'semi_200': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'fralusopark_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Cheated | Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "        ],\n",
    "        'independent': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Cheated, train and test are from lbfgs, 0.01, 5000 | Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "        ],\n",
    "        'independent_200': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'fralusopark',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def explain(train_path, test_path, experiment, param_set, logs_file, dry_run=False):\n",
    "    pickle_file = '{}/{}/{}/{}.pkl'.format(PICKLES_PATH, experiment, param_set['dataset'], generate_suffix(param_set))\n",
    "    if os.path.isfile(pickle_file):\n",
    "        with open(pickle_file, 'rb') as filename:\n",
    "            model = pickle.load(filename)\n",
    "    else:\n",
    "        logs_file.write(\"WARNING: No pickle found @ {}. Skipping...\\n\".format(pickle_file))\n",
    "        return None\n",
    "\n",
    "    if not os.path.isfile(train_path):\n",
    "        logs_file.write(\"WARNING: No train file found @ {}. Skipping...\\n\".format(train_path))\n",
    "        return train_path\n",
    "    if not os.path.isfile(test_path):\n",
    "        logs_file.write(\"WARNING: No test file found @ {}. Skipping...\\n\".format(test_path))\n",
    "        return test_path\n",
    "\n",
    "    train = None\n",
    "    test = None\n",
    "    try:\n",
    "        train = pd.read_csv(train_path, sep=\";\")\n",
    "    except Exception as e:\n",
    "        logs_file.write('Test file reading error: ' + str(e))\n",
    "    try:\n",
    "        test = pd.read_csv(test_path , sep=\";\")\n",
    "    except Exception as e:\n",
    "        logs_file.write('Test file reading error: ' + str(e))\n",
    "                \n",
    "\n",
    "    train = train[train.columns.difference(['frameTime', 'label', 'name'])]\n",
    "\n",
    "    patients = list(set(test['name']))\n",
    "    feature_names = [feature for feature in list(test.columns) if feature not in ['label', 'name', 'frameTime']]\n",
    "    \n",
    "    output_dic = {'name': [], 'hc': [], 'pd': [], 'features': []}\n",
    "    output_csv = pd.DataFrame(output_dic)\n",
    "    \n",
    "    try:\n",
    "        for patient in patients:\n",
    "            patient_lines = test[test[\"name\"] == patient]\n",
    "            patient_lines = patient_lines[patient_lines.columns.difference(['frameTime', 'label', 'name'])]\n",
    "\n",
    "            output_path_html = \"{}/{}/{}/{}_{}.html\".format(EXPLANATIONS_PATH, experiment, param_set['dataset'], patient.split('_')[0], generate_suffix(param_set))\n",
    "\n",
    "            if os.path.isfile(output_path_html):\n",
    "                print(\"Explanation exists, ignoring...\")\n",
    "                continue\n",
    "\n",
    "            logs_file.write(\"Explaining patient {} to {}\\n\".format(patient.split('_')[0], output_path_html))\n",
    "            if not dry_run:\n",
    "                print(patient)\n",
    "                try:\n",
    "                    patient_explanation = explain_patient(experiment, patient, model, patient_lines, train.to_numpy(), feature_names, param_set, output_path=output_path_html)\n",
    "                    output_csv = output_csv.append(patient_explanation, ignore_index=True)\n",
    "                except:\n",
    "                    logs_file.write('Error explaining patient {} for experiment {}\\n'.format(patient, experiment))\n",
    "    except Exception:\n",
    "        output_csv.to_csv('/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/csv/{}/{}'.format(experiment, generate_suffix(param_set)), index=False)\n",
    "        message = traceback.format_exc()\n",
    "        logs_file.write(message + '\\n')\n",
    "        print(message)\n",
    "    print(output_csv)\n",
    "    output_csv.to_csv('/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/csv/{}/{}.csv'.format(experiment, generate_suffix(param_set)), index=False)\n",
    "    f = open(\"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/done_experiments/{}.txt\".format(experiment), \"w\")\n",
    "    f.write(\"Done\")\n",
    "    f.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_path(experiment, param_set):\n",
    "    train = \"{}/{}/{}/{}_train.csv\".format(SUBSETS_PATH, experiment, param_set['dataset'], generate_suffix(param_set))\n",
    "    test =  \"{}/{}/{}/{}_test.csv\".format (SUBSETS_PATH, experiment, param_set['dataset'], generate_suffix(param_set))  \n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_names():\n",
    "    return {\n",
    "        'baseline': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'baseline_200': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'independent': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'independent_200': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'semi': ['fralusopark_gita', 'fralusopark_mdvr_kcl', 'gita_fralusopark', 'gita_mdvr_kcl', 'mdvr_kcl_fralusopark', 'mdvr_kcl_gita'],\n",
    "        'semi_200': ['fralusopark_gita', 'fralusopark_mdvr_kcl', 'gita_fralusopark', 'gita_mdvr_kcl', 'mdvr_kcl_fralusopark', 'mdvr_kcl_gita']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_patients(experiment, mlp_params):\n",
    "    missing_models = []\n",
    "    f = open(\"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/logs_{}.txt\".format(experiment), \"w\")\n",
    "    for param_set in mlp_params:\n",
    "        train_path, test_path = get_train_test_path(experiment, param_set)\n",
    "        result = explain(train_path, test_path, experiment, param_set, f)\n",
    "        \n",
    "        if result:\n",
    "            missing_models.append(result)\n",
    "    f.close()\n",
    "      \n",
    "    if missing_models != []:\n",
    "        f = open('/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/missing_modules/{}.txt'.format(experiment), \"w\")\n",
    "        f.write('\\n'.join(missing_models))\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlp_params = generate_mlp_params_list(dry_run=True)\n",
    "\n",
    "    experiment_processes = []\n",
    "    for experiment in mlp_params.keys():\n",
    "        experiment_processes.append(Process(target=run_experiment_patients, args=(experiment, mlp_params[experiment],)))\n",
    "\n",
    "    for experiment_process in experiment_processes:\n",
    "        experiment_process.start()\n",
    "\n",
    "    for experiment_process in experiment_processes:\n",
    "        experiment_process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVPEPUDEAC0018_readtext\n",
      "AVPEPUDEA0010_readtext\n",
      "AVPEPUDEAC0057_readtext\n",
      "AVPEPUDEAC0054_readtext\n",
      "AVPEPUDEA0050_readtext\n",
      "AVPEPUDEAC0041_readtext\n",
      "AVPEPUDEA0030_readtext\n",
      "AVPEPUDEAC0051_readtext\n",
      "AVPEPUDEA0059_readtext\n",
      "AVPEPUDEAC0014_readtext\n",
      "AVPEPUDEA0008_readtext\n",
      "AVPEPUDEA0002_readtext\n",
      "AVPEPUDEAC0017_readtext\n",
      "AVPEPUDEA0013_readtext\n",
      "AVPEPUDEAC0052_readtext\n",
      "AVPEPUDEAC0047_readtext\n",
      "AVPEPUDEA0039_readtext\n",
      "AVPEPUDEAC0023_readtext\n",
      "AVPEPUDEA0020_readtext\n",
      "AVPEPUDEAC0006_readtext\n",
      "AVPEPUDEAC0044_readtext\n",
      "AVPEPUDEAC0020_readtext\n",
      "AVPEPUDEAC0010_readtext\n",
      "AVPEPUDEAC0005_readtext\n",
      "AVPEPUDEA0015_readtext\n",
      "AVPEPUDEA0024_readtext\n",
      "AVPEPUDEA0057_readtext\n",
      "AVPEPUDEAC0026_readtext\n",
      "AVPEPUDEA0022_readtext\n",
      "AVPEPUDEAC0024_readtext\n",
      "AVPEPUDEAC0003_readtext\n",
      "AVPEPUDEAC0022_readtext\n",
      "AVPEPUDEA0016_readtext\n",
      "AVPEPUDEA0035_readtext\n",
      "AVPEPUDEA0043_readtext\n",
      "AVPEPUDEA0045_readtext\n",
      "AVPEPUDEA0009_readtext\n",
      "AVPEPUDEAC0011_readtext\n",
      "AVPEPUDEAC0048_readtext\n",
      "AVPEPUDEAC0039_readtext\n",
      "AVPEPUDEA0026_readtext\n",
      "AVPEPUDEAC0016_readtext\n",
      "AVPEPUDEAC0019_readtext\n",
      "AVPEPUDEA0046_readtext\n",
      "AVPEPUDEAC0008_readtext\n",
      "AVPEPUDEAC0046_readtext\n",
      "AVPEPUDEAC0013_readtext\n",
      "AVPEPUDEA0041_readtext\n",
      "AVPEPUDEA0058_readtext\n",
      "AVPEPUDEA0051_readtext\n",
      "AVPEPUDEA0054_readtext\n",
      "AVPEPUDEAC0049_readtext\n",
      "AVPEPUDEAC0050_readtext\n",
      "AVPEPUDEA0006_readtext\n",
      "AVPEPUDEA0034_readtext\n",
      "AVPEPUDEAC0021_readtext\n",
      "AVPEPUDEA0023_readtext\n",
      "AVPEPUDEA0007_readtext\n",
      "AVPEPUDEAC0027_readtext\n",
      "AVPEPUDEA0031_readtext\n",
      "AVPEPUDEA0011_readtext\n",
      "AVPEPUDEAC0029_readtext\n",
      "AVPEPUDEA0049_readtext\n",
      "AVPEPUDEAC0012_readtext\n",
      "AVPEPUDEAC0031_readtext\n",
      "AVPEPUDEA0005_readtext\n",
      "AVPEPUDEA0014_readtext\n",
      "AVPEPUDEA0021_readtext\n",
      "AVPEPUDEA0056_readtext\n",
      "AVPEPUDEA0052_readtext\n",
      "AVPEPUDEA0055_readtext\n",
      "AVPEPUDEA0042_readtext\n",
      "AVPEPUDEA0003_readtext\n",
      "AVPEPUDEA0017_readtext\n",
      "AVPEPUDEA0032_readtext\n",
      "AVPEPUDEAC0040_readtext\n",
      "AVPEPUDEA0001_readtext\n",
      "AVPEPUDEA0053_readtext\n",
      "AVPEPUDEAC0053_readtext\n",
      "AVPEPUDEA0025_readtext\n",
      "AVPEPUDEAC0025_readtext\n",
      "AVPEPUDEAC0034_readtext\n",
      "AVPEPUDEAC0030_readtext\n",
      "AVPEPUDEAC0045_readtext\n",
      "AVPEPUDEA0048_readtext\n",
      "AVPEPUDEAC0015_readtext\n",
      "AVPEPUDEAC0042_readtext\n",
      "AVPEPUDEA0047_readtext\n",
      "AVPEPUDEAC0037_readtext\n",
      "AVPEPUDEAC0043_readtext\n",
      "AVPEPUDEAC0035_readtext\n",
      "AVPEPUDEAC0007_readtext\n",
      "AVPEPUDEA0038_readtext\n",
      "AVPEPUDEA0029_readtext\n",
      "AVPEPUDEAC0028_readtext\n",
      "AVPEPUDEA0027_readtext\n",
      "AVPEPUDEA0037_readtext\n",
      "AVPEPUDEAC0033_readtext\n",
      "AVPEPUDEAC0001_readtext\n",
      "AVPEPUDEAC0004_readtext\n",
      "Empty DataFrame\n",
      "Columns: [name, hc, pd, features]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
