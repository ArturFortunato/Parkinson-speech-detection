{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (0.2.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/lib64/python3.6/site-packages (from lime) (0.23.2)\n",
      "Requirement already satisfied: tqdm in /usr/lib/python3.6/site-packages (from lime) (4.45.0)\n",
      "Requirement already satisfied: scipy in /usr/lib64/python3.6/site-packages (from lime) (1.3.3)\n",
      "Requirement already satisfied: matplotlib in /usr/lib64/python3.6/site-packages (from lime) (2.2.5)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from lime) (0.17.2)\n",
      "Requirement already satisfied: numpy in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from lime) (1.19.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.6/site-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3.6/site-packages (from matplotlib->lime) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from matplotlib->lime) (2.8.1)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3.6/site-packages (from matplotlib->lime) (2020.5)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3.6/site-packages (from matplotlib->lime) (1.14.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib64/python3.6/site-packages (from matplotlib->lime) (1.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2.5.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2020.9.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/lib64/python3.6/site-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2.9.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/lib64/python3.6/site-packages (from scikit-image>=0.12->lime) (7.2.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
      "Requirement already satisfied: tabulate in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (0.8.9)\n",
      "Requirement already satisfied: pdfkit in /afs/l2f.inesc-id.pt/home/aof/.local/lib/python3.6/site-packages (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lime --user\n",
    "!pip install tabulate --user\n",
    "!pip install pdfkit --user\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "\n",
    "from lime import lime_tabular\n",
    "\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lime\n",
    "import pickle\n",
    "\n",
    "PICKLES_PATH = \"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/classification/pickles\"\n",
    "SUBSETS_PATH = \"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/subsets_useful\"\n",
    "EXPLANATIONS_PATH = \"/afs/inesc-id.pt/home/aof/public_html\"\n",
    "\n",
    "BOOTSTRAP = '<head><link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\" integrity=\"sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO\" crossorigin=\"anonymous\"></head>'\n",
    "\n",
    "EXPLANATIONS = {\n",
    "    'f0': 'The number of open/close cycles of the glottis.',\n",
    "    'hnr': 'Measures the frequency variation between cycles. Affected by lack of control on the vocal cords vibration.',\n",
    "    'jitter': 'Measures the amplitude variation between cycles.',\n",
    "    'shimmer': 'The ratio between periodic (associated with normal speed production) and non-periodic (associated with noise) speech components.',\n",
    "    'mfcc': 'Features that approximate to our perception of the audio quality.',\n",
    "    'plp': 'Features that approximate to our perception of the audio quality.',  \n",
    "}\n",
    "\n",
    "\n",
    "NORMAL_VALUES = {\n",
    "    'f0': '105-160 (male) | 175-245 (Female)',\n",
    "    'hnr': '< 1.04',\n",
    "    'jitter': '< 3.81',\n",
    "    'shimmer': '< 40',\n",
    "    'mfcc': '',\n",
    "    'plp': '',  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_html_to_file(html, path):\n",
    "    f = open(path, \"w\")\n",
    "    f.write(html)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_explanation_text(feature):\n",
    "    return get_explanation_or_values(feature, EXPLANATIONS)\n",
    "\n",
    "def get_normal_values(feature):\n",
    "    return get_explanation_or_values(feature, NORMAL_VALUES)\n",
    "\n",
    "def get_explanation_or_values(feature, obj):\n",
    "    lower = feature.lower()\n",
    "    \n",
    "    for key in obj:\n",
    "        if key in lower:\n",
    "            return obj[key]\n",
    "    \n",
    "    print(\"No explanation or normal value found for feature {}\".format(feature))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from classification.mlp import MLP\n",
    "import traceback\n",
    "\n",
    "'''\n",
    "    Model: MLPClassifier instance\n",
    "    Patient_Lines: lines of CSV corresponding to the patient to be tested\n",
    "    Train: Training fractionf of the CSV\n",
    "    Feature_names: list of features (columns of csv)\n",
    "    Output path: file path to write the output, None for writing on screen\n",
    "'''\n",
    "def explain_patient(experiment, patient, model, patient_lines, train, feature_names, params, pickle_path, output_path=None):\n",
    "    # Classify using model and ignore if it doesn't match label in patient lines\n",
    "    mlp = MLP(None,None,None,None,model=model)\n",
    "    try:\n",
    "        diagnostic_correct = mlp.score(patient_lines, 0.5, None, '')\n",
    "        if not diagnostic_correct:\n",
    "            return []\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        return []\n",
    "    \n",
    "    num_features = 5\n",
    "    patient_lines = patient_lines[patient_lines.columns.difference(['frameTime', 'label', 'name'])]\n",
    "\n",
    "    #Build tabular\n",
    "    try:\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=feature_names, class_names=['HC', 'PD'], discretize_continuous=False)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # Pass each line to LimeTabularExplainer\n",
    "    explanations = []\n",
    "    for i, row in patient_lines.iterrows():\n",
    "        #Run only 1 in every 10 time frames (10 samples / second)\n",
    "        if i % 10 == 0:\n",
    "            try:\n",
    "                explanations.append(explainer.explain_instance(row, model.predict_proba, num_features=num_features, top_labels=1))\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                return []\n",
    "\n",
    "    # Sum all local_exp\n",
    "    total_local_exp = {}\n",
    "    for explanation in explanations:\n",
    "        #local_exp = {1: [(12, 0.139704949653412), (44, 0.10278682378094679)]}\n",
    "        local_exp = explanation.local_exp\n",
    "        # will produce {<feature>: (count, sum_weight), ...\n",
    "        for tup in local_exp[0 if 0 in local_exp else 1]:\n",
    "            #tup = (12, 0.139704949653412)\n",
    "            feature = feature_names[tup[0]]\n",
    "            if feature in total_local_exp:\n",
    "                total_local_exp[feature][0] += 1\n",
    "                total_local_exp[feature][1] += tup[1]\n",
    "            else:\n",
    "                total_local_exp[feature] = [1, tup[1]]\n",
    "        \n",
    "    #Average local_exp\n",
    "    # Key --> feature\n",
    "    # value --> average value\n",
    "    averaged_local_exp = {}\n",
    "\n",
    "    for key in total_local_exp.keys():\n",
    "        tup = total_local_exp[key]\n",
    "        averaged_local_exp[key] = tup[1] / tup[0]\n",
    "    \n",
    "    # Normalize predict_proba\n",
    "    # [P(HC), P(PD)]\n",
    "    average_predict_proba = [sum(items) for items in zip(*[explanation.predict_proba for explanation in explanations ])]\n",
    "    average_predict_proba /= np.float64(sum(average_predict_proba))\n",
    "\n",
    "    # Each feature contribution represents its contribution to \n",
    "    # Classify the patient as PD. Therefore, suming them all will \n",
    "    # Add to P(PD). By dividing it by P(PD) we will project this\n",
    "    # In the [-1, 1] interval\n",
    "    prob_pd = average_predict_proba[1]\n",
    "    for key in averaged_local_exp:\n",
    "        averaged_local_exp[key] /= prob_pd\n",
    "\n",
    "    # Get mean values for all features\n",
    "    average_feature_values = {}\n",
    "    for column in patient_lines:\n",
    "        average_feature_values[column] = patient_lines[column].mean()\n",
    "        \n",
    "    result = display_or_save_patient(averaged_local_exp, average_predict_proba, average_feature_values, output_path, num_features, patient)\n",
    "\n",
    "    f = open(\"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/done_patients/{}/{}_{}.txt\".format(experiment, generate_suffix(params), patient.split(\"_\")[0]), \"w\")\n",
    "    f.write(\"Done\")\n",
    "    f.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suffix(params):\n",
    "    return \"{}_{}_{}_{}\".format(params['solver'], params['alpha'], params['max_iter'], params['activation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "def display_or_save_patient(averaged_local_exp, average_predict_proba, average_feature_values, output_path, num_features, patient):\n",
    "    prediction_proba = \\\n",
    "            '<table class=\"table\" style=\"margin: 0 auto;\"> \\\n",
    "                <thead> \\\n",
    "                    <tr> \\\n",
    "                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">HC</td> \\\n",
    "                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">PD</td> \\\n",
    "                    </tr> \\\n",
    "                </thead> \\\n",
    "                <tr> \\\n",
    "                    <td style=\"margin: 2px; text-align: center;\">{}</td> \\\n",
    "                    <td style=\"margin: 2px; text-align: center;\">{}</td> \\\n",
    "                </tr> \\\n",
    "            </table>' \\\n",
    "            .format(\"{:.2f}\".format(average_predict_proba[0]), \"{:.2f}\".format(average_predict_proba[1]))\n",
    "    \n",
    "    averaged_local_exp = {k: v for k, v in sorted(averaged_local_exp.items(), key=lambda item: -abs(item[1]))}\n",
    "    feature_weights = averaged_local_exp.items()\n",
    "    feature_weight_table = '<table class=\"table\" style=\"margin: 0 auto;\"> \\\n",
    "                                <thead> \\\n",
    "                                    <tr> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Feature</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">PD probability contribution</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Subject\\'s average value</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Healthy values</td> \\\n",
    "                                        <td style=\"margin: 2px; text-align:center; font-weight: bold;\">Description</td> \\\n",
    "                                    </tr> \\\n",
    "                                </thead>'\n",
    "\n",
    "    accounted = 0\n",
    "    for feature_weight in feature_weights:\n",
    "        s = '<tr> \\\n",
    "                <td style=\"margin: 2px; text-align:center; font-weight: bold;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "                <td style=\"margin: 2px; text-align:center;\">{}</td> \\\n",
    "            </tr>' \\\n",
    "            .format(feature_weight[0], feature_weight[1], average_feature_values[feature_weight[0]], get_normal_values(feature_weight[0]), get_explanation_text(feature_weight[0]))\n",
    "        feature_weight_table += s\n",
    "        \n",
    "        accounted += 1\n",
    "        if accounted == num_features:\n",
    "            break\n",
    "\n",
    "    feature_weight_table += '</table>'\n",
    "    \n",
    "    #for feature_index in averaged_local_exp:\n",
    "    #    feature_weight.append([feature_index, averaged_local_exp[feature_index]])\n",
    "    \n",
    "    \n",
    "    table = '<table class=\"table\" style=\"margin: 0 auto;\"> \\\n",
    "                <thead> \\\n",
    "                    <tr> \\\n",
    "                        <td style=\"width: 45%; margin: 2px; text-align:center; font-weight: bold;\">Prediction Probabilities</td> \\\n",
    "                        <td style=\"width: 45%; margin: 2px; text-align:center; font-weight: bold;\">Feature weight</td> \\\n",
    "                    </tr> \\\n",
    "                </thead> \\\n",
    "                <tr> \\\n",
    "                    <td style=\"width: 45%; margin: 2px; text-align:center\">{}</td> \\\n",
    "                    <td style=\"width: 45%; margin: 2px; text-align:center\">{}</td> \\\n",
    "                </tr> \\\n",
    "            </table>' \\\n",
    "        .format(prediction_proba, feature_weight_table)\n",
    "\n",
    "    if output_path == None:\n",
    "        display(HTML(table))\n",
    "    else:\n",
    "        write_html_to_file(BOOTSTRAP + table, output_path)\n",
    "        \n",
    "    dic = {'name': patient.split(\"_\")[0], 'hc': average_predict_proba[0], 'pd': average_predict_proba[1], 'features': [json.dumps(averaged_local_exp)]}\n",
    "    results = pd.DataFrame(dic, index=None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlp_params_list(dry_run=False):\n",
    "    if (dry_run):\n",
    "        return {\n",
    "            \"independent_200\": [\n",
    "                { # Got it\n",
    "                    'dataset': 'gita',\n",
    "                    'solver': 'lbfgs',\n",
    "                    'activation': 'tanh',\n",
    "                    'alpha': 0.0001,\n",
    "                    'max_iter': 2000,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    return {\n",
    "        \"baseline\": [\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "        ],\n",
    "        'baseline_200': [\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "        ],\n",
    "        'semi': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'fralusopark_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita_fralusopark',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita_fralusopark',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita_fralusopark',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "        ],\n",
    "        'semi_200': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'fralusopark_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Cheated | Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'mdvr_kcl_gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "        ],\n",
    "        'independent': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Cheated, train and test are from lbfgs, 0.01, 5000 | Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'adam',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "        ],\n",
    "        'independent_200': [\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.01,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "            { # Got it, on the hlt machine\n",
    "                'dataset': 'gita',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.001,\n",
    "                'max_iter': 2000,\n",
    "            },\n",
    "            { # Got it\n",
    "                'dataset': 'fralusopark',\n",
    "                'solver': 'lbfgs',\n",
    "                'activation': 'tanh',\n",
    "                'alpha': 0.0001,\n",
    "                'max_iter': 5000,\n",
    "            },\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def explain(train_path, test_path, experiment, param_set, logs_file, dry_run=False):\n",
    "    pickle_file = '{}/{}/{}/{}.pkl'.format(PICKLES_PATH, experiment, param_set['dataset'], generate_suffix(param_set))\n",
    "    if os.path.isfile(pickle_file):\n",
    "        with open(pickle_file, 'rb') as filename:\n",
    "            model = pickle.load(filename)\n",
    "    else:\n",
    "        logs_file.write(\"WARNING: No pickle found @ {}. Skipping...\\n\".format(pickle_file))\n",
    "        return None\n",
    "\n",
    "    if not os.path.isfile(train_path):\n",
    "        logs_file.write(\"WARNING: No train file found @ {}. Skipping...\\n\".format(train_path))\n",
    "        return train_path\n",
    "    if not os.path.isfile(test_path):\n",
    "        logs_file.write(\"WARNING: No test file found @ {}. Skipping...\\n\".format(test_path))\n",
    "        return test_path\n",
    "\n",
    "    train = None\n",
    "    test = None\n",
    "    try:\n",
    "        train = pd.read_csv(train_path, sep=\";\")\n",
    "    except Exception as e:\n",
    "        logs_file.write('Test file reading error: ' + str(e))\n",
    "    try:\n",
    "        test = pd.read_csv(test_path , sep=\";\")\n",
    "    except Exception as e:\n",
    "        logs_file.write('Test file reading error: ' + str(e))\n",
    "                \n",
    "\n",
    "    train = train[train.columns.difference(['frameTime', 'label', 'name'])]\n",
    "\n",
    "    patients = list(set(test['name']))\n",
    "    feature_names = [feature for feature in list(test.columns) if feature not in ['label', 'name', 'frameTime']]\n",
    "    \n",
    "    output_dic = {'name': [], 'hc': [], 'pd': [], 'features': []}\n",
    "    output_csv = pd.DataFrame(output_dic)\n",
    "    \n",
    "    try:\n",
    "        for patient in patients:\n",
    "            patient_lines = test[test[\"name\"] == patient]\n",
    "            #patient_lines = patient_lines[patient_lines.columns.difference(['frameTime', 'label', 'name'])]\n",
    "\n",
    "            output_path_html = \"{}/{}/{}/{}_{}.html\".format(EXPLANATIONS_PATH, experiment, param_set['dataset'], patient.split('_')[0], generate_suffix(param_set))\n",
    "\n",
    "            if os.path.isfile(output_path_html):\n",
    "                print(\"Explanation exists, ignoring...\")\n",
    "                continue\n",
    "\n",
    "            logs_file.write(\"Explaining patient {} to {}\\n\".format(patient.split('_')[0], output_path_html))\n",
    "\n",
    "            try:\n",
    "                patient_explanation = explain_patient(experiment, patient, model, patient_lines, train.to_numpy(), feature_names, param_set, pickle_file, output_path=output_path_html)\n",
    "                output_csv = output_csv.append(patient_explanation, ignore_index=True)\n",
    "            except:\n",
    "                logs_file.write('Error explaining patient {} for experiment {}\\n'.format(patient, experiment))\n",
    "    except Exception:\n",
    "        output_csv.to_csv('/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/csv/{}/{}'.format(experiment, generate_suffix(param_set)), index=False)\n",
    "        message = traceback.format_exc()\n",
    "        logs_file.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "    output_csv.to_csv('/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/csv/{}/{}.csv'.format(experiment, generate_suffix(param_set)), index=False)\n",
    "    f = open(\"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/done_experiments/{}.txt\".format(experiment), \"w\")\n",
    "    f.write(\"Done\\n\")\n",
    "    f.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_path(experiment, param_set):\n",
    "    train = \"{}/{}/{}/{}_train.csv\".format(SUBSETS_PATH, experiment, param_set['dataset'], generate_suffix(param_set))\n",
    "    test =  \"{}/{}/{}/{}_test.csv\".format (SUBSETS_PATH, experiment, param_set['dataset'], generate_suffix(param_set))  \n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_names():\n",
    "    return {\n",
    "        'baseline': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'baseline_200': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'independent': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'independent_200': ['fralusopark', 'gita', 'mdvr_kcl'],\n",
    "        'semi': ['fralusopark_gita', 'fralusopark_mdvr_kcl', 'gita_fralusopark', 'gita_mdvr_kcl', 'mdvr_kcl_fralusopark', 'mdvr_kcl_gita'],\n",
    "        'semi_200': ['fralusopark_gita', 'fralusopark_mdvr_kcl', 'gita_fralusopark', 'gita_mdvr_kcl', 'mdvr_kcl_fralusopark', 'mdvr_kcl_gita']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_patients(experiment, mlp_params):\n",
    "    missing_models = []\n",
    "    f = open(\"/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/logs_{}.txt\".format(experiment), \"w\")\n",
    "    for param_set in mlp_params:\n",
    "        train_path, test_path = get_train_test_path(experiment, param_set)\n",
    "        result = explain(train_path, test_path, experiment, param_set, f, dry_run=False)\n",
    "        \n",
    "        if result:\n",
    "            missing_models.append(result)\n",
    "    f.close()\n",
    "      \n",
    "    if missing_models != []:\n",
    "        f = open('/afs/inesc-id.pt/home/aof/thesis/Parkinson-speech-detection/explainability/logs/missing_modules/{}.txt'.format(experiment), \"w\")\n",
    "        f.write('\\n'.join(missing_models))\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlp_params = generate_mlp_params_list(dry_run=False)\n",
    "\n",
    "    experiment_processes = []\n",
    "    for experiment in mlp_params.keys():\n",
    "        experiment_processes.append(Process(target=run_experiment_patients, args=(experiment, mlp_params[experiment],)))\n",
    "\n",
    "    for experiment_process in experiment_processes:\n",
    "        experiment_process.start()\n",
    "\n",
    "    for experiment_process in experiment_processes:\n",
    "        experiment_process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
