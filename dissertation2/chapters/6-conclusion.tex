  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- coding: utf-8; mode: latex -*- %%
  %
%%%%%                       CHAPTER
 %%%
  %

\chapter{Conclusions}
%\addcontentsline{lof}{chapter}{\thechapter\quad Nihil Molestiae}
%\addcontentsline{lot}{chapter}{\thechapter\quad Nihil Molestiae}
\label{ch:magna}

%\begin{quotation}
%  {\small\it Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

%{\small\it -- Cerico}
%\end{quotation}
This work addressed two issues of the \gls{pd} diagnosis task: universality and explainability. Firstly, lack of training data for \gls{pd} diagnostic creates a necessity for pre-trained models that can be re-trained with a small dataset of speech from a new language and be able to diagnose patients speaking the new language. Secondly, lack of ability to understand black-box models' diagnosis is a barrier to real-world usage of such models, which can be solved using explainability models. \\
First, we evaluated the performance of a new language-independent model for the \gls{pd} diagnosis task. Baseline results (training and testing the model with speech from subjects speaking the same language) achieved a maximum accuracy of 90\% with the two neural network architectures tested. An intermediate step was taken between the baseline and a language-independent model, in which models were trained with one dataset and with 90\% of another dataset (with different language speakers) and tested with the remaining 10\% of the second dataset. Both architectures yielded a maximum accuracy of 90\% in the intermediate setting, without loosing performance compared to the baseline. This demonstrated the capacity of these models to be applied to a different language with a reduced amount of training data when pre-trained with a different language. This characteristic can be useful as the size of the available datasets limits the quality of the training. Although results of the present work were very promising, the percentage of the new language dataset used for training (90\%) is still high. Reducing the amount of data to re-train a model is worth investigating in the future. When training a language-independent model (trained with two datasets and tested with a different one), accuracy dropped to two thirds for both architectures. Although results were inferior to the state-of-the-art regarding the accuracy metric, a similar work achieved an accuracy of 77\% with a language-independent model \cite{parkinson_three_languages}. Our model with highest accuracy yielded a maximum value of 76\% on the recall metric, significantly higher than the 53\% achieved by the work of Orozco-Arroyave, \textit{et al.} (2016). Therefore, the present work produces less false negatives (\gls{pd} subjects classified as \gls{hc}), thus being a more robust tool to support the medical activity. \\
In the second part of the work, the LIME model was used to generate an explanation for each diagnostic. This step allowed to explain the classification results in a way understandable by medical professionals, thus providing trust in the model. This explanation can foster the adoption of computational diagnostic models to be used in clinical scenarios, as these models often produce more accurate diagnostics than medical professionals. The LIME explanation report indicated the probability of each subject belonging to on of the classes (\gls{pd} and \gls{hc}), as well as the top five acoustic features which contribute the most to the model's classification. For each feature, the contribution weight and the subject's average value were also included, together with the range of values of a healthy patient and a small description of the feature. This report largely extends the information produced by the classification model, which only indicates the final diagnostic, thus providing the medical professional with information that allows to make an informed diagnostic. Finally, a global analysis was conducted to evaluate the average contribution of each acoustic feature extracted and the percentage of test subjects for which each feature was one of the top five with the highest contribution. Combining both results, we concluded that MFCC and PLP features represent better information for the \gls{pd} diagnostic task than \gls{f0}, jitter, shimmer, and HNR. Note that both MFCC and PLP are abstract mathematical representations of sound, and are therefore difficult to explain to a medical professional. Additionally, to the best of our knowledge, there is no known range of values for both MFCC and PLP parameters that defines a healthy patient, which prevents our model to generate a complete report on these features. This work should be extended in the future with simpler, easy to understand features in order for the model to be used in a clinical scenario. \\
There are several paths to continue this work. \\
First, the current pipeline presents some limitations that should be addressed. As previously described, there are complexity limitations associated with abstract features, such as \gls{plp}s and \gls{mfcc}s. Using simpler features, such as Logarithmic Filter Banks (instead of MFCC), would increase the clarity, and therefore the trustworthiness/reliability, of the model's diagnostic. In addition, graphical representations of the physical manifestation of each feature can be added to the explanation. The normal values for some features, such as \gls{f0}, depend on meta features (the normal values for \gls{f0} for males is range between 105 and 160 \textbf{Hz} and between 175 to 245 \textit{Hz} for females). Thus, adding the gender as a feature for the model could help improve the model's performance. \\
Both the classification and the explanation pipeline's steps can be further improved. First, the similarity between the average contribution (weight) of all features on the explanation model suggests some correlation between features. This hypothesis can be further studied, using a model to evaluate the interactions between features, such as factorization machines. Detecting redundant features could help reduce the model's complexity, thus reducing resource requirements. Also, the results achieved on the semi language-independent experiments showed performance was not affected when training a model with two languages. Further analysis on the impact of varying the training percentage of the test language would shed light into the relation between data quantity used to re-train a model and the possible performance loss. Moreover, this work focused on explaining the diagnosis of each patient. A study on the global contributions of each feature could clarify their individual importance to the \gls{pd} classification task. This could be achieved by generating global explanations using LIME, or by using models such as \gls{cav}s \cite{TCAV} or \gls{nam}s \cite{NAM}. Finally, both for the classification and the explanation steps, different models can be used to make a comparative analysis. This would allow to both assess the classification ability of multiple models, and to compare the explanations generated by various models and the trust provided to the medical professionals. \\
The goal of generating explanations is to provide the medical professionals with a tool that can shed light into the \textit{black-box} classification models. Thus, these models should be tested in real-world scenarios, to rate their adequacy to perform this task. During the real-world evaluation, a comparative analysis could be conducted between explainability models, in order to assess which ones provide more trust to the end-users of the product (the medical professionals). This can be done by generating explanations for the same user using different explainability models and assessing the degree of confidence of the medical professional in each one of them. This evaluation could also lead to the conclusion that a combination of both methods provides more information, which would provide a higher level of trust by the medical professional on the classification models. Feature types (such as audio ou images) should also be compared, as to understand which are better accepted by medical professionals. For example, the explanations generated by the model developed during this work could be compared with the ones produced by the work described on section 3.3, in which LIME was used to explain \gls{pd} diagnostic with SPECT DaTSCAN images of the brain.
  %
 %%%
%%%%%                           THE END
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "tese"
%%% End: 
