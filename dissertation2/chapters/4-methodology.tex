  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- coding: utf-8; mode: latex -*- %%
  %
%%%%%                       CHAPTER
 %%%
  %

\chapter{Experimental Setup}
%\addcontentsline{lof}{chapter}{\thechapter\quad Nihil Molestiae}
%\addcontentsline{lot}{chapter}{\thechapter\quad Nihil Molestiae}
\label{ch:adipisci}

%\begin{quotation}
%  {\small\it Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

%{\small\it -- Cerico}
%\end{quotation}

This section describes the methodology. First, the \textit{corpora} used in this work are described, followed by the approaches to be taken (feature selection, classification model, explanation generation model, and multi-language tests). Finally, the evaluation proceedings are presented.Figure \ref{pipeline} shows the pipeline for the system's architecture. 

\section{Corpus Description}

As described in section 3.3, most datasets available for this task are very small, thus being insufficient to train neural models \cite{underfitting_small_datasets}. Nevertheless, a small number of common speech production tasks are available in the datasets. As the differences between some of these datasets are only the model of the microphone used for recording and the language of the test subjects, several datasets may be combined to produce sufficiently long collections of data that to be used for neural models \cite{parkinson_braga}, \cite{parkinson_acoustic_despotovic}, \cite{parkinson_phonemic_relevance}, \cite{x_vector_parkinson}. Different datasets were used for training and testing, or to combine instances from different datasets in the training and/or testing sets \cite{parkinson_three_languages}, all proving to be accurate in the \gls{pd} classification task.

In this study, we used 3 datasets for training and testing the model -- FraLusoPark \cite{fralusopark}, GITA \cite{GITA}, and Mobile Device Voice Recordings at King's College London (MDVR\_KCL) \cite{MDVR}.

The FraLusoPark dataset is composed by speech from 120 patients, half of which are native French speakers and the other half are European Portuguese speakers. The dataset also contains 120 healthy participants as a control group (with the same distribution between French and European Portuguese speakers as the \gls{pd} participants). Each group of \gls{pd} patients is divided into three subgroups, based on the number of years since diagnostic: 20 early stage patients (who have been diagnosed less than 3 years before and present no motor fluctuations), 20 medium stage patients (with a diagnostic made 4 to 9 years before the study, or less than 3 years and experiencing motor fluctuations), and 20 advanced stage patients, diagnosed over 10 years ago. The patients are recorded twice for every speech production task, BEFORE (at least 12 hours after medication) and AFTER medication (at least 1 hour after medication). FraLusoPark participants were asked to perform a set of speech production tasks: sustain the vowel \textit{\/a\/} at a steady pitch, maximum phonation time of the vowel \textit{\/a\/} on a single breath, \gls{ddk} (repetition of the pseudo-word \textit{\/pa-ta-ka\/} for a fast rate during 30 seconds), reading aloud 10 words and 10 sentences, formed by adapting part of section V.2 of the Frenchay Dysarthria Assessment of Intelligibility (FDA-2), reading of a short text (adapted to French and European Portuguese), storytelling by guided visual stimuli, reading a collection of sentences with specific language-dependent prosodic properties and free conversation for 3 minutes. In the scope of this study, we will only consider the Portuguese speakers of this dataset, as it was not possible to access the audios from the french patients. These recordings total 62 minutes and 45 seconds for \gls{pd} patients' and 51 minutes and 2 seconds for \gls{hc} participants'.

The GITA dataset contains recordings of 50 \gls{pd} patients and 50 \gls{hc}, evenly distributed between genders. For the \gls{pd} group, the average age is 62.2 $\pm$ 11.2 years for male participants and 60.1 $\pm$ 7.8 for female participants. Considering the \gls{hc} group, the average age is 61.2 $\pm$ 11.3 years for male participants and 60.7 $\pm$ 7.7 for female participants. Multiple stages of disease progression are considered in this study (time since diagnostic ranges between 0.4 - 20 years for male patients and 1 - 41 years for female patients). All the participants are Colombian Spanish native speakers. Recordings of the \gls{pd} patients were made no more than 3 hours after the morning medication. Different speech production tasks were performed to examine phonation, articulation and prosody. To analyze phonation, participants were asked to sustain the five Spanish vowels and to repeat the same five vowels, but alternating the tone between low and high. Regarding articulation, a \gls{ddk} evaluation was performed with the pseudo-words \textit{/pa-ta-ka/}, \textit{/pa-ka-ta/} and \textit{/pe-ta-ka/}. Finally, for the evaluation of prosody, both \gls{pd} patients and \gls{hc} were asked to repeat a series of sentences with different levels of complexity, to read a dialogue between a doctor and a patient (this text contained the complete set of Spanish sounds), to read sentences with a strong emphasis on a set of words and freely speak about their daily routine. These recordings total 15 minutes and 31 seconds for \gls{pd} patients' and 14 minutes and 41 seconds for \gls{hc} participants'.

Lastly, the MDVR\_KCL dataset was recorded in the context of phone calls, in an acoustically controlled environment. The dataset contains 16 participants with \gls{pd} (11 male and 4 female) and 21 \gls{hc} (3 male and 18 female), totaling 37 English speakers. The \gls{pd} group contains patients from all the stages of the disease (early, mid and late stages) according to the  Hoehn and Yahr scale \cite{hoehn_yahr}. The participants were asked to read a text (``The north wind and the sun'' or ``Tech. Engin. Computer applications in geography snippet''). Additionally, the text executor started a spontaneous conversation with each participant about various topics.

To homogenize the datasets, only the text-reading tasks were considered herein. This yields a total of 131 \gls{hc} and 125 \gls{pd} speakers of European Portuguese, Colombian Spanish, and English. 

\section{Data Processing}

The original audio files contained full interviews of each test subject, therefore requiring segmentation in order to remove useless audio fragments. Firstly, silence between speech segments was removed. Next, sounds produced by the subject that were not considered as speech were also delete. Finally, any audio segments containing speech from other participants in the interview (interviewers) were also eliminated.
\\
After data processing, FralusoPark recordings totaled XX minutes and YY seconds for \gls{pd} patients and XX minutes and YY seconds for \gls{hc} participants. Gita recordings yielded a total of ZZZ minutes and AA seconds for \gls{pd} subjects and XX minutes and YY seconds for \gls{hc} participants. Finally, MDVR\_KCL recordings totaled XX minutes and YY seconds for \gls{pd} patients and XX minutes and YY seconds for \gls{hc} subjects.

\section{Feature Extraction}

In order to extract the features, the openSMILE \cite{openSMILE} tool was used. To extract the complete set of features, four configurations were used: \textit{MFCC12\_0\_D\_A.conf} for MFCC's, \textit{PLP\_0\_D\_A.conf} to extract \gls{plp}'s and \textit{prosodyAcf2.conf}, for prosody features (\gls{f0} and \gls{hnr}).
\\
OpenSMILE was configured to use a sliding window of 25\textit{ms} with a frame step of 10\textit{ms}. After the extraction, each participant was represented by a list of frames, each frame described by a list of features. For the classification of each patient, the resulting diagnostic is obtained by averaging the model's output for each of the patient's frames.

\section{Classification Experiments}

Three distinct experiments have been conducted during the present work. First, to create a baseline, a classification model was trained with each training set individually and tested with the corresponding testing set (i.e., both from the same database). This first approach allowed to score the classification model without considering multiple languages. Secondly, the same model was trained to evaluate its performance as a semi language-independent classifier. For this, the model was trained using one complete dataset and with a fraction of another dataset (for this work, this fraction represented 90\% of the number of subjects in the dataset), thus combining two languages in the same training set. The model was then tested with the remaining percentage (10\%) of the second dataset. All the combinations between the three datasets were tested, leading to 6 dataset combinations. By testing this semi language-independent version, it was possible to evaluate an intermediate step between a language-dependent and a language-independent classification model, shedding light into the model's sensitivity to the language. Lastly, a completely language-independent model was trained with combination of two datasets. For this last experiment, each model was trained with two datasets and tested with the third, thus allowing to evaluate the model's ability to diagnose a patient who speaks in a language different than the ones used to train the model.
\\
For these experiments, the \textit{scikit-learn} implementation \cite{scikit-learn} of a \gls{mlp} was used. Two different architectures were tested to evaluate their ability to learn from the training data. The first architecture contains one entry layer with \textbf{N} neurons (where \textbf{N} is the number of input features), a fully-connected hidden layer with \textbf{N + 1} neurons and an output layer with 1 neuron, whose value represents the probability of the test subject to be classified as \gls{pd}. \textbf{Refer the papers where they say this model is great}. The second architecture also contains an input layer with \textbf{N} neurons, two fully-connected hidden layers, comprising 200 neurons each and, similarly to the first architecture, an output layer with 1 neuron, also representing the probability of the subject under evaluation to be diagnosed with \gls{pd}. For there experiments, the breaking point between \gls{hc} and \gls{pd} diagnostic (the output value of the neuron from the output layer) was set to 0.5 (50\%).
\\
In order to find the best model configuration, the experiments were repeated testing multiple values for \textit{alpha} ($ 10^{-3} $, $ 10 ^{-2} $, $ 10 ^{-1} $), maximum number of iterations (1000, 2000, 5000) and solver for weight optimization (lbfgs, sgd, adam). 

\section{Explanation Production}

After the classification experiments, explanations were generated for each individual of the test set (with all models described in section 4.4). As the objective of this work was to generate an explanation for each diagnostic individually, a mixed model was used. By selecting a mixed model, this work can be extended to generate global explanations, thus shedding light on the importance of each acoustic feature for the model's diagnostic.
\\
The selected model was LIME. This model yielded results on explaining \gls{pd} diagnostics with SPECT DaTSCAN images of the brain that were confirmed by the bibliography. This work aimed to verify if the same performance can be achieved using acoustic features.
\\
To explain the diagnostic of one subject, the \textit{explain\_instance} method from the \textit{LimeTabularExplainer} class was used with each feature list (each representing a time frame). Next, two operations were performed. First, the classification model's output was averaged between all time frames, creating a final classification probability for the subject. Secondly, each feature weight (which LIME calculated) was also averaged, thus creating a final weight for each feature (from which the top 5 features with larger contribution to the classification were selected and displayed). To this report, a list of values considered to be normal for each one of the features was added. Finally, to assist the interpretation of the report by the medical professional, a small description of each feature was also added. The complete list of normal values and descriptions can be found on tables \ref{normalValues} and \ref{featureDescription}, respectively.

\section{Model Evaluation}

In order to evaluate the classification model's performance, multiple metrics have been selected, namely:

\begin{itemize}
	\item ~\textit{accuracy} (which allows to evaluate the percentage of subjects correctly diagnosed)
	\item ~\textit{precision} (that yields the fraction of subjects diagnosed with \gls{pd} that were correctly classified)
	\item ~\textit{recall} (that evaluates the percentage of \gls{pd} subjects that were correctly diagnosed)
	\item ~\textit{F1-score} (that allows to evaluate \textit{precision} and \textit{recall} in the same metric)
	\item ~\textit{Specificity} (which assess the ratio of subjects classified as \gls{hc} that were correctly diagnosed)
\end{itemize}
\\
These metrics shed light on the performance of the models, which allows to determine the best parameters and architecture. Furthermore, \textit{recall} allows to evaluate the percentage of subjects from the \gls{pd} group that were correctly diagnosed, which, combined with specificity (that evaluates the number of subjects from the \gls{pd} group wrongly diagnosed), provides model trust information to medical professionals.
\\
To assess LIME's performance, average values of each feature were extracted from the bibliography (see \textbf{text}) and are shown along with the values in each explanation, in order to compare the each subject's feature values with the it's range for a healthy individual. This comparison will allow to evaluate the model's ability to find abnormal values (or their absence) and select those features as justifications for a certain classification.

\begin{figure*}[t]
	\begin{center}
		\includegraphics[clip=true, width=\textwidth]{figs/pipeline.png}
	\end{center}
	\caption{Pipeline of the proposed model.}
	\label{pipeline}
\end{figure*}

\pagebreak